{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "xfRgJ5Ibiltt",
        "outputId": "62bc78da-6be9-4937-f6a4-e85c9e59e415"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:10: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:10: SyntaxWarning: invalid escape sequence '\\s'\n",
            "/tmp/ipython-input-3883962383.py:10: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  statlog_df = pd.read_csv(statlog_url, sep='\\s+', header=None)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'Cardiovascular_Disease_Dataset.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3883962383.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m datasets = {\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# 'Heart_Disease_Prediction (1).csv': pd.read_csv('Heart_Disease_Prediction (1).csv'), # This file was not found\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0;34m'Cardiovascular_Disease_Dataset.csv'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cardiovascular_Disease_Dataset.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;34m'hear_LAPPt.csv'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hear_LAPPt.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;34m'heart nandal.csv'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'heart nandal.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Cardiovascular_Disease_Dataset.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "import joblib\n",
        "\n",
        "statlog_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/heart/heart.dat\"\n",
        "statlog_df = pd.read_csv(statlog_url, sep='\\s+', header=None)\n",
        "statlog_columns = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg',\n",
        "                   'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target']\n",
        "statlog_df.columns = statlog_columns\n",
        "statlog_df['target'] = statlog_df['target'].apply(lambda x: 1 if x == 2 else 0)\n",
        "\n",
        "\n",
        "datasets = {\n",
        "    # 'Heart_Disease_Prediction (1).csv': pd.read_csv('Heart_Disease_Prediction (1).csv'), # This file was not found\n",
        "    'Cardiovascular_Disease_Dataset.csv': pd.read_csv('Cardiovascular_Disease_Dataset.csv'),\n",
        "    'hear_LAPPt.csv': pd.read_csv('hear_LAPPt.csv'),\n",
        "    'heart nandal.csv': pd.read_csv('heart nandal.csv'),\n",
        "    'statlog': statlog_df\n",
        "}\n",
        "\n",
        "\n",
        "standard_columns = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg',\n",
        "                    'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'target']\n",
        "column_mappings = {\n",
        "    'Heart_Disease_Prediction (1).csv': {\n",
        "        'Age': 'age', 'Sex': 'sex', 'Chest pain type': 'cp', 'BP': 'trestbps',\n",
        "        'Cholesterol': 'chol', 'FBS over 120': 'fbs', 'EKG results': 'restecg',\n",
        "        'Max HR': 'thalach', 'Exercise angina': 'exang', 'ST depression': 'oldpeak',\n",
        "        'Slope of ST': 'slope', 'Number of vessels fluro': 'ca', 'Heart Disease': 'target'\n",
        "    },\n",
        "    'Cardiovascular_Disease_Dataset.csv': {\n",
        "        'age': 'age', 'gender': 'sex', 'chestpain': 'cp', 'restingBP': 'trestbps',\n",
        "        'serumcholestrol': 'chol', 'fastingbloodsugar': 'fbs', 'restingrelectro': 'restecg',\n",
        "        'maxheartrate': 'thalach', 'exerciseangia': 'exang', 'oldpeak': 'oldpeak',\n",
        "        'slope': 'slope', 'noofmajorvessels': 'ca', 'target': 'target'\n",
        "    },\n",
        "    'heart nandal.csv': {\n",
        "        'trtbps': 'trestbps', 'thalachh': 'thalach', 'exng': 'exang', 'slp': 'slope',\n",
        "        'caa': 'ca', 'output': 'target'\n",
        "    },\n",
        "    'hear_LAPPt.csv': {\n",
        "        'age': 'age', 'sex': 'sex', 'cp': 'cp', 'trestbps': 'trestbps',\n",
        "        'chol': 'chol', 'fbs': 'fbs', 'restecg': 'restecg', 'thalach': 'thalach',\n",
        "        'exang': 'exang', 'oldpeak': 'oldpeak', 'slope': 'slope', 'ca': 'ca',\n",
        "        'target': 'target'\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "for name, df in datasets.items():\n",
        "    if name in column_mappings:\n",
        "        df.rename(columns=column_mappings[name], inplace=True)\n",
        "    datasets[name] = df[[col for col in standard_columns if col in df.columns]]\n",
        "\n",
        "combined_df = pd.concat(datasets.values(), ignore_index=True)\n",
        "initial_rows = combined_df.shape[0]\n",
        "combined_df = combined_df.drop_duplicates()\n",
        "print(f\"Rows before removing duplicates: {initial_rows}, after: {combined_df.shape[0]}\")\n",
        "\n",
        "\n",
        "numerical_cols = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'ca']\n",
        "categorical_cols = ['cp', 'restecg', 'slope']\n",
        "other_cols = ['sex', 'fbs', 'exang']\n",
        "for col in numerical_cols:\n",
        "    combined_df[col] = combined_df[col].fillna(combined_df[col].mean())\n",
        "for col in categorical_cols + other_cols:\n",
        "    combined_df[col] = combined_df[col].fillna(combined_df[col].mode()[0])\n",
        "\n",
        "\n",
        "combined_df['target'] = combined_df['target'].apply(lambda x: 1 if x in ['Presence', 1, '1'] else 0 if x in [0, '0', 'Absence'] else np.nan)\n",
        "combined_df = combined_df.dropna(subset=['target'])\n",
        "\n",
        "\n",
        "combined_df = pd.get_dummies(combined_df, columns=categorical_cols, drop_first=True)\n",
        "scaler = MinMaxScaler()\n",
        "combined_df[numerical_cols] = scaler.fit_transform(combined_df[numerical_cols])\n",
        "combined_df['target'] = combined_df['target'].astype(int)\n",
        "\n",
        "\n",
        "X = combined_df.drop('target', axis=1)\n",
        "y = combined_df['target']\n",
        "rf_temp = RandomForestClassifier(n_estimators=300, max_depth=30, random_state=8412)\n",
        "rf_temp.fit(X, y)\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': rf_temp.feature_importances_\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "top_features = feature_importance['Feature'].head(12).tolist()  # Top 12 features\n",
        "print(\"Top 12 Features:\", top_features)\n",
        "\n",
        "\n",
        "X_top = X[top_features]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_top, y, test_size=0.3, random_state=8412)\n",
        "\n",
        "\n",
        "rf_model = RandomForestClassifier(n_estimators=300, max_depth=30, min_samples_split=2, class_weight='balanced', random_state=8412)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_pred_train = rf_model.predict(X_train)\n",
        "print(\"\\nRandom Forest Train Metrics (12 features):\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_train, y_pred_train))\n",
        "print(f\"Train Accuracy: {accuracy_score(y_train, y_pred_train) * 100:.2f}%\")\n",
        "print(f\"Precision: {precision_score(y_train, y_pred_train) * 100:.2f}%\")\n",
        "print(f\"Recall: {recall_score(y_train, y_pred_train) * 100:.2f}%\")\n",
        "print(f\"F1-Score: {f1_score(y_train, y_pred_train) * 100:.2f}%\")\n",
        "\n",
        "\n",
        "y_pred_test = rf_model.predict(X_test)\n",
        "print(\"\\nRandom Forest Test Metrics (12 features):\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_test))\n",
        "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_test) * 100:.2f}%\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred_test) * 100:.2f}%\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred_test) * 100:.2f}%\")\n",
        "print(f\"F1-Score: {f1_score(y_test, y_pred_test) * 100:.2f}%\")\n",
        "\n",
        "\n",
        "cv_recall_rf = cross_val_score(rf_model, X_train, y_train, cv=5, scoring='recall')\n",
        "print(f\"CV Recall: {cv_recall_rf.mean():.4f} ± {cv_recall_rf.std():.4f}\")\n",
        "\n",
        "\n",
        "seeds = [44, 57, 98, 76]\n",
        "for seed in seeds:\n",
        "    np.random.seed(seed)\n",
        "    sample_indices = np.random.choice(X_test.index, 500, replace=False)\n",
        "    X_sample = X_test.loc[sample_indices]\n",
        "    y_sample_actual = y_test[sample_indices]\n",
        "    y_sample_pred = rf_model.predict(X_sample)\n",
        "    print(f\"Accuracy on 500 samples (seed {seed}): {accuracy_score(y_sample_actual, y_sample_pred) * 100:.2f}%\")\n",
        "\n",
        "\n",
        "np.random.seed(44)\n",
        "sample_indices = np.random.choice(X_test.index, 500, replace=False)\n",
        "X_sample = X_test.loc[sample_indices]\n",
        "y_sample_actual = y_test[sample_indices]\n",
        "y_sample_pred = rf_model.predict(X_sample)\n",
        "comparison_df = X_sample[['thalach', 'oldpeak', 'ca']].copy()\n",
        "comparison_df['Actual'] = y_sample_actual\n",
        "comparison_df['Predicted'] = y_sample_pred\n",
        "comparison_df.to_csv('sample_predictions_500.csv', index=False)\n",
        "print(f\"\\nAccuracy on 500 samples: {accuracy_score(y_sample_actual, y_sample_pred) * 100:.2f}%\")\n",
        "print(\"\\nSample Predictions (first 10):\")\n",
        "print(comparison_df.head(10))\n",
        "\n",
        "\n",
        "joblib.dump(rf_model, 'rf_model.pkl')\n",
        "metrics = {\n",
        "    'Train Accuracy': accuracy_score(y_train, y_pred_train) * 100,\n",
        "    'Train Precision': precision_score(y_train, y_pred_train) * 100,\n",
        "    'Train Recall': recall_score(y_train, y_pred_train) * 100,\n",
        "    'Train F1-Score': f1_score(y_train, y_pred_train) * 100,\n",
        "    'Test Accuracy': accuracy_score(y_test, y_pred_test) * 100,\n",
        "    'Test Precision': precision_score(y_test, y_pred_test) * 100,\n",
        "    'Test Recall': recall_score(y_test, y_pred_test) * 100,\n",
        "    'Test F1-Score': f1_score(y_test, y_pred_test) * 100,\n",
        "    'CV Recall': cv_recall_rf.mean(),\n",
        "    'CV Recall Std': cv_recall_rf.std(),\n",
        "    'False Negatives': confusion_matrix(y_test, y_pred_test)[1, 0]\n",
        "}\n",
        "pd.DataFrame([metrics]).to_csv('model_results.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "JvO2g-fFQe6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_features = feature_importance['Feature'].head(12).tolist()\n",
        "X_top = X[top_features]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_top, y, test_size=0.3, random_state=8412)\n",
        "rf_model = RandomForestClassifier(n_estimators=300, max_depth=30, min_samples_split=2, class_weight='balanced', random_state=8412)\n",
        "rf_model.fit(X_train, y_train)\n",
        "cv_recall_rf = cross_val_score(rf_model, X_train, y_train, cv=5, scoring='recall')\n",
        "print(f\"CV Recall with 12 features: {cv_recall_rf.mean():.4f} ± {cv_recall_rf.std():.4f}\")"
      ],
      "metadata": {
        "id": "bMqJnK1_1yEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=300, random_state=8412)\n",
        "rfe = RFE(estimator=rf, n_features_to_select=12)\n",
        "rfe.fit(X_train, y_train)\n",
        "selected_features = X_train.columns[rfe.support_].tolist()\n",
        "print(selected_features)"
      ],
      "metadata": {
        "id": "BwDp2_L0FEgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# From confusion matrix\n",
        "cm = [[228, 29], [22, 283]]\n",
        "accuracy = (cm[0][0] + cm[1][1]) / (cm[0][0] + cm[0][1] + cm[1][0] + cm[1][1])\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")  # Output: 0.9093"
      ],
      "metadata": {
        "id": "Ue05q9rVFMOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d301Rab0FPRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(77)\n",
        "sample_indices = np.random.choice(X_test.index, 500, replace=False)\n",
        "X_sample = X_test.loc[sample_indices]\n",
        "y_sample_actual = y_test[sample_indices]\n",
        "y_sample_pred = rf_model.predict(X_sample)\n",
        "\n",
        "available_features = ['thalach', 'oldpeak', 'ca']\n",
        "comparison_df = X_sample[available_features].copy()\n",
        "comparison_df['Actual'] = y_sample_actual\n",
        "comparison_df['Predicted'] = y_sample_pred\n",
        "\n",
        "accuracy_sample = accuracy_score(y_sample_actual, y_sample_pred)\n",
        "print(f\"Accuracy on 500 samples: {accuracy_sample * 100:.2f}%\")\n",
        "print(\"\\nSample Predictions (first 10):\")\n",
        "print(comparison_df.head(10))\n",
        "comparison_df.to_csv('sample_predictions_500.csv', index=False)"
      ],
      "metadata": {
        "id": "oOGBAP-0phvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "print(\"\\nRandom Forest Metrics (12 features):\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_rf))\n",
        "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_rf) * 100:.2f}%\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred_rf) * 100:.2f}%\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred_rf) * 100:.2f}%\")\n",
        "print(f\"F1-Score: {f1_score(y_test, y_pred_rf) * 100:.2f}%\")\n",
        "seeds = [44, 57, 98, 76]\n",
        "for seed in seeds:\n",
        "    np.random.seed(seed)\n",
        "    sample_indices = np.random.choice(X_test.index, 500, replace=False)\n",
        "    X_sample = X_test.loc[sample_indices]\n",
        "    y_sample_actual = y_test[sample_indices]\n",
        "    y_sample_pred = rf_model.predict(X_sample)\n",
        "    print(f\"Accuracy on 500 samples (seed {seed}): {accuracy_score(y_sample_actual, y_sample_pred) * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "eQ0CkXsgy6yO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "import joblib\n",
        "\n",
        "# Add UCI Statlog dataset\n",
        "statlog_url = \"\"https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/heart/heart.dat\n",
        "statlog_df = pd.read_csv(statlog_url, sep='\\s+', header=None)\n",
        "statlog_columns = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg',\n",
        "                   'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target']\n",
        "statlog_df.columns = statlog_columns\n",
        "statlog_df['target'] = statlog_df['target'].apply(lambda x: 1 if x == 2 else 0)\n",
        "\n",
        "# Current datasets\n",
        "datasets = {\n",
        "    'Heart_Disease_Prediction (1).csv': pd.read_csv('Heart_Disease_Prediction (1).csv'),\n",
        "    'Cardiovascular_Disease_Dataset.csv': pd.read_csv('Cardiovascular_Disease_Dataset.csv'),\n",
        "    'hear_LAPPt.csv': pd.read_csv('hear_LAPPt.csv'),\n",
        "    'heart nandal.csv': pd.read_csv('heart nandal.csv'),\n",
        "    'statlog': statlog_df\n",
        "}\n",
        "\n",
        "# Standard columns (exclude thal)\n",
        "standard_columns = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg',\n",
        "                    'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'target']\n",
        "column_mappings = {\n",
        "    'Heart_Disease_Prediction (1).csv': {\n",
        "        'Age': 'age', 'Sex': 'sex', 'Chest pain type': 'cp', 'BP': 'trestbps',\n",
        "        'Cholesterol': 'chol', 'FBS over 120': 'fbs', 'EKG results': 'restecg',\n",
        "        'Max HR': 'thalach', 'Exercise angina': 'exang', 'ST depression': 'oldpeak',\n",
        "        'Slope of ST': 'slope', 'Number of vessels fluro': 'ca', 'Heart Disease': 'target'\n",
        "    },\n",
        "    'Cardiovascular_Disease_Dataset.csv': {\n",
        "        'age': 'age', 'gender': 'sex', 'chestpain': 'cp', 'restingBP': 'trestbps',\n",
        "        'serumcholestrol': 'chol', 'fastingbloodsugar': 'fbs', 'restingrelectro': 'restecg',\n",
        "        'maxheartrate': 'thalach', 'exerciseangia': 'exang', 'oldpeak': 'oldpeak',\n",
        "        'slope': 'slope', 'noofmajorvessels': 'ca', 'target': 'target'\n",
        "    },\n",
        "    'heart nandal.csv': {\n",
        "        'trtbps': 'trestbps', 'thalachh': 'thalach', 'exng': 'exang', 'slp': 'slope',\n",
        "        'caa': 'ca', 'output': 'target'\n",
        "    },\n",
        "    'hear_LAPPt.csv': {\n",
        "        'age': 'age', 'sex': 'sex', 'cp': 'cp', 'trestbps': 'trestbps',\n",
        "        'chol': 'chol', 'fbs': 'fbs', 'restecg': 'restecg', 'thalach': 'thalach',\n",
        "        'exang': 'exang', 'oldpeak': 'oldpeak', 'slope': 'slope', 'ca': 'ca',\n",
        "        'target': 'target'\n",
        "    }\n",
        "}\n",
        "\n",
        "# Preprocess\n",
        "for name, df in datasets.items():\n",
        "    if name in column_mappings:\n",
        "        df.rename(columns=column_mappings[name], inplace=True)\n",
        "    datasets[name] = df[[col for col in standard_columns if col in df.columns]]\n",
        "\n",
        "combined_df = pd.concat(datasets.values(), ignore_index=True)\n",
        "initial_rows = combined_df.shape[0]\n",
        "combined_df = combined_df.drop_duplicates()\n",
        "print(f\"Rows before removing duplicates: {initial_rows}, after: {combined_df.shape[0]}\")\n",
        "\n",
        "# Impute missing values\n",
        "numerical_cols = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'ca']\n",
        "categorical_cols = ['cp', 'restecg', 'slope']\n",
        "other_cols = ['sex', 'fbs', 'exang']\n",
        "for col in numerical_cols:\n",
        "    combined_df[col] = combined_df[col].fillna(combined_df[col].mean())\n",
        "for col in categorical_cols + other_cols:\n",
        "    combined_df[col] = combined_df[col].fillna(combined_df[col].mode()[0])\n",
        "\n",
        "# Standardize target\n",
        "combined_df['target'] = combined_df['target'].apply(lambda x: 1 if x in ['Presence', 1, '1'] else 0 if x in [0, '0', 'Absence'] else np.nan)\n",
        "combined_df = combined_df.dropna(subset=['target'])\n",
        "\n",
        "# Preprocessing\n",
        "combined_df = pd.get_dummies(combined_df, columns=categorical_cols, drop_first=True)\n",
        "scaler = MinMaxScaler()\n",
        "combined_df[numerical_cols] = scaler.fit_transform(combined_df[numerical_cols])\n",
        "combined_df['target'] = combined_df['target'].astype(int)\n",
        "\n",
        "# Feature selection\n",
        "X = combined_df.drop('target', axis=1)\n",
        "y = combined_df['target']\n",
        "rf_temp = RandomForestClassifier(n_estimators=200, max_depth=30, random_state=8412)\n",
        "rf_temp.fit(X, y)\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': rf_temp.feature_importances_\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "top_features = feature_importance['Feature'].head(12).tolist()  # Top 12 features\n",
        "print(\"Top 12 Features:\", top_features)\n",
        "\n",
        "# Train-test split\n",
        "X_top = X[top_features]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_top, y, test_size=0.3, random_state=8412)\n",
        "\n",
        "# Random Forest with best params\n",
        "rf_model = RandomForestClassifier(n_estimators=200, max_depth=30, min_samples_split=2, class_weight='balanced', random_state=8412)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Train metrics\n",
        "y_pred_train = rf_model.predict(X_train)\n",
        "print(\"\\nRandom Forest Train Metrics (12 features):\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_train, y_pred_train))\n",
        "print(f\"Train Accuracy: {accuracy_score(y_train, y_pred_train) * 100:.2f}%\")\n",
        "print(f\"Precision: {precision_score(y_train, y_pred_train) * 100:.2f}%\")\n",
        "print(f\"Recall: {recall_score(y_train, y_pred_train) * 100:.2f}%\")\n",
        "print(f\"F1-Score: {f1_score(y_train, y_pred_train) * 100:.2f}%\")\n",
        "\n",
        "# Test metrics\n",
        "y_pred_test = rf_model.predict(X_test)\n",
        "print(\"\\nRandom Forest Test Metrics (12 features):\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_test))\n",
        "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_test) * 100:.2f}%\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred_test) * 100:.2f}%\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred_test) * 100:.2f}%\")\n",
        "print(f\"F1-Score: {f1_score(y_test, y_pred_test) * 100:.2f}%\")\n",
        "\n",
        "# CV Recall\n",
        "cv_recall_rf = cross_val_score(rf_model, X_train, y_train, cv=5, scoring='recall')\n",
        "print(f\"CV Recall: {cv_recall_rf.mean():.4f} ± {cv_recall_rf.std():.4f}\")\n",
        "\n",
        "# Sample accuracy (500 samples)\n",
        "seeds = [44, 57, 98, 76]\n",
        "for seed in seeds:\n",
        "    np.random.seed(seed)\n",
        "    sample_indices = np.random.choice(X_test.index, 500, replace=False)\n",
        "    X_sample = X_test.loc[sample_indices]\n",
        "    y_sample_actual = y_test[sample_indices]\n",
        "    y_sample_pred = rf_model.predict(X_sample)\n",
        "    print(f\"Accuracy on 500 samples (seed {seed}): {accuracy_score(y_sample_actual, y_sample_pred) * 100:.2f}%\")\n",
        "\n",
        "# Save sample predictions\n",
        "np.random.seed(44)\n",
        "sample_indices = np.random.choice(X_test.index, 500, replace=False)\n",
        "X_sample = X_test.loc[sample_indices]\n",
        "y_sample_actual = y_test[sample_indices]\n",
        "y_sample_pred = rf_model.predict(X_sample)\n",
        "comparison_df = X_sample[['thalach', 'oldpeak', 'ca']].copy()\n",
        "comparison_df['Actual'] = y_sample_actual\n",
        "comparison_df['Predicted'] = y_sample_pred\n",
        "comparison_df.to_csv('sample_predictions_500.csv', index=False)\n",
        "print(f\"\\nAccuracy on 500 samples: {accuracy_score(y_sample_actual, y_sample_pred) * 100:.2f}%\")\n",
        "print(\"\\nSample Predictions (first 10):\")\n",
        "print(comparison_df.head(10))\n",
        "\n",
        "# Save model and metrics\n",
        "joblib.dump(rf_model, 'rf_model.pkl')\n",
        "metrics = {\n",
        "    'Train Accuracy': accuracy_score(y_train, y_pred_train) * 100,\n",
        "    'Train Precision': precision_score(y_train, y_pred_train) * 100,\n",
        "    'Train Recall': recall_score(y_train, y_pred_train) * 100,\n",
        "    'Train F1-Score': f1_score(y_train, y_pred_train) * 100,\n",
        "    'Test Accuracy': accuracy_score(y_test, y_pred_test) * 100,\n",
        "    'Test Precision': precision_score(y_test, y_pred_test) * 100,\n",
        "    'Test Recall': recall_score(y_test, y_pred_test) * 100,\n",
        "    'Test F1-Score': f1_score(y_test, y_pred_test) * 100,\n",
        "    'CV Recall': cv_recall_rf.mean(),\n",
        "    'CV Recall Std': cv_recall_rf.std(),\n",
        "    'False Negatives': confusion_matrix(y_test, y_pred_test)[1, 0]\n",
        "}\n",
        "pd.DataFrame([metrics]).to_csv('model_results.csv', index=False)"
      ],
      "metadata": {
        "id": "8XFRafvV4VOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SHAP plot\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "explainer = shap.TreeExplainer(rf_model)\n",
        "shap_values = explainer.shap_values(X_test)\n",
        "plt.figure()\n",
        "shap.summary_plot(shap_values, X_test, feature_names=top_features, show=False)\n"
      ],
      "metadata": {
        "id": "xOgiiNZEvB7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas numpy scikit-learn matplotlib seaborn"
      ],
      "metadata": {
        "id": "Jb5cP6NAyc-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion matrix heatmap\n",
        "import seaborn as sns\n",
        "y_pred = rf_model.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "            xticklabels=['No Disease', 'Disease'], yticklabels=['No Disease', 'Disease'])\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n"
      ],
      "metadata": {
        "id": "Gsqet7CLydta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature importance bar plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(top_features, rf_model.feature_importances_, color='skyblue')\n",
        "plt.xlabel('Feature Importance')\n",
        "plt.title('Top 12 Feature Importance')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "81rZxFZUyxqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ROC curve\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "y_pred_proba = rf_model.predict_proba(X_test)[:, 1]\n",
        "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
        "auc = roc_auc_score(y_test, y_pred_proba)\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc:.2f})', color='blue')\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RmovF9aE1XKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ej85Ucc1SAZZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}